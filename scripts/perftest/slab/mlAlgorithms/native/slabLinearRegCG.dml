#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT SOLVES LINEAR REGRESSION USING THE CONJUGATE GRADIENT ALGORITHM
#



# Parameters
intercept_status = 2;     # 0: no intercept, 1: add intercept, 2: add intercept, shift & rescale
tolerance = 0.000001;      # Tolerance for convergence
max_iteration = 100;       # Maximum number of iterations
regularization = 0.000001; # Regularization constant

print ("BEGIN LINEAR REGRESSION SCRIPT");

# Generate data internally
n = $1; # number of rows
m = 100;  # number of columns
X = rand(rows=n, cols=m, min=0, max=1, sparsity=0.9, seed=42)
y = rand(rows=n, cols=1, min=0, max=1, sparsity=0.9, seed=24)

sum_x = sum(X)
sum_y = sum(y)
for (ix in 1:5) {


    ones_n = matrix(1, rows = n, cols = 1)
    zero_cell = matrix(0, rows = 1, cols = 1)

    # Introduce the intercept, shift and rescale the columns of X if needed
    m_ext = m
    if (intercept_status == 1 | intercept_status == 2) { # add the intercept column
        X = cbind(X, ones_n)
        m_ext = ncol(X)
    }

    scale_lambda = matrix(1, rows = m_ext, cols = 1)
    if (intercept_status == 1 | intercept_status == 2) {
        scale_lambda[m_ext, 1] = 0
    }

    if (intercept_status == 2) {  # scale-&-shift X columns to mean 0, variance 1
        avg_X_cols = t(colSums(X)) / n
        var_X_cols = (t(colSums(X ^ 2)) - n * (avg_X_cols ^ 2)) / (n - 1)
        is_unsafe = (var_X_cols <= 0)
        scale_X = 1.0 / sqrt(var_X_cols * (1 - is_unsafe) + is_unsafe)
        scale_X[m_ext, 1] = 1
        shift_X = -avg_X_cols * scale_X
        shift_X[m_ext, 1] = 0
    } else {
        scale_X = matrix(1, rows = m_ext, cols = 1)
        shift_X = matrix(0, rows = m_ext, cols = 1)
    }

    lambda = scale_lambda * regularization
    beta_unscaled = matrix(0, rows = m_ext, cols = 1)

    if (max_iteration == 0) {
        max_iteration = m_ext
    }
    i = 0

    # BEGIN THE CONJUGATE GRADIENT ALGORITHM
    print ("Running the CG algorithm...")

    r = -t(X) %*% y

    if (intercept_status == 2) {
        r = scale_X * r + shift_X %*% r[m_ext, ]
    }

    p = -r
    norm_r2 = sum(r ^ 2)
    norm_r2_initial = norm_r2
    norm_r2_target = norm_r2_initial * tolerance ^ 2
    print ("||r|| initial value = " + sqrt(norm_r2_initial) + ",  target value = " + sqrt(norm_r2_target))

    while (i < max_iteration & norm_r2 > norm_r2_target) {
        if (intercept_status == 2) {
            ssX_p = scale_X * p
            ssX_p[m_ext, ] = ssX_p[m_ext, ] + t(shift_X) %*% p
        } else {
            ssX_p = p
        }

        q = t(X) %*% (X %*% ssX_p)

        if (intercept_status == 2) {
            q = scale_X * q + shift_X %*% q[m_ext, ]
        }

        q = q + lambda * p
        a = norm_r2 / sum(p * q)
        beta_unscaled = beta_unscaled + a * p
        r = r + a * q
        old_norm_r2 = norm_r2
        norm_r2 = sum(r ^ 2)
        p = -r + (norm_r2 / old_norm_r2) * p
        i = i + 1
        print ("Iteration " + i + ":  ||r|| / ||r init|| = " + sqrt(norm_r2 / norm_r2_initial))
    }

    if (i >= max_iteration) {
        print ("Warning: the maximum number of iterations has been reached.")
    }
    print ("The CG algorithm is done.")
    # END THE CONJUGATE GRADIENT ALGORITHM

    if (intercept_status == 2) {
        beta = scale_X * beta_unscaled
        beta[m_ext, ] = beta[m_ext, ] + t(shift_X) %*% beta_unscaled
    } else {
        beta = beta_unscaled
    }

    print ("Computing the statistics...")

    avg_tot = sum(y) / n
    ss_tot = sum(y ^ 2)
    ss_avg_tot = ss_tot - n * avg_tot ^ 2
    var_tot = ss_avg_tot / (n - 1)
    y_residual = y - X %*% beta
    avg_res = sum(y_residual) / n
    ss_res = sum(y_residual ^ 2)
    ss_avg_res = ss_res - n * avg_res ^ 2

    R2 = 1 - ss_res / ss_avg_tot
    if (n > m_ext) {
        dispersion  = ss_res / (n - m_ext)
        adjusted_R2 = 1 - dispersion / (ss_avg_tot / (n - 1))
    } else {
        dispersion  = 0.0 / 0.0
        adjusted_R2 = 0.0 / 0.0
    }

    R2_nobias = 1 - ss_avg_res / ss_avg_tot
    deg_freedom = n - m - 1
    if (deg_freedom > 0) {
        var_res = ss_avg_res / deg_freedom
        adjusted_R2_nobias = 1 - var_res / (ss_avg_tot / (n - 1))
    } else {
        var_res = 0.0 / 0.0
        adjusted_R2_nobias = 0.0 / 0.0
        print ("Warning: zero or negative number of degrees of freedom.")
    }

    R2_vs_0 = 1 - ss_res / ss_tot
    if (n > m) {
        adjusted_R2_vs_0 = 1 - (ss_res / (n - m)) / (ss_tot / n)
    } else {
        adjusted_R2_vs_0 = 0.0 / 0.0
    }

    str = "AVG_TOT_Y," + avg_tot;                                    #  Average of the response value Y
    str = append (str, "STDEV_TOT_Y," + sqrt (var_tot));             #  Standard Deviation of the response value Y
    str = append (str, "AVG_RES_Y," + avg_res);                      #  Average of the residual Y - pred(Y|X), i.e. residual bias
    str = append (str, "STDEV_RES_Y," + sqrt (var_res));             #  Standard Deviation of the residual Y - pred(Y|X)
    str = append (str, "DISPERSION," + dispersion);                  #  GLM-style dispersion, i.e. residual sum of squares / # d.f.
    str = append (str, "R2," + R2);                                  #  R^2 of residual with bias included vs. total average
    str = append (str, "ADJUSTED_R2," + adjusted_R2);                #  Adjusted R^2 of residual with bias included vs. total average
    str = append (str, "R2_NOBIAS," + R2_nobias);                    #  R^2 of residual with bias subtracted vs. total average
    str = append (str, "ADJUSTED_R2_NOBIAS," + adjusted_R2_nobias);  #  Adjusted R^2 of residual with bias subtracted vs. total average
    if (intercept_status == 0) {
        str = append (str, "R2_VS_0," + R2_vs_0);                    #  R^2 of residual with bias included vs. zero constant
        str = append (str, "ADJUSTED_R2_VS_0," + adjusted_R2_vs_0);  #  Adjusted R^2 of residual with bias included vs. zero constant
    }

    print (str);


}


